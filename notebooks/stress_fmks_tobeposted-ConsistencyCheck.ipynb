{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Effective Stiffness of Composite Material\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This example uses the `MKSHomogenizationModel` to create a homogenization linkage for the effective stiffness. This example starts with a brief background of the homogenization theory on the components of the effective elastic stiffness tensor for a composite material. Then the example generates random microstructures and their average stress values that will be used to show how to calibrate and use our model. We will also show how to use tools from [sklearn](http://scikit-learn.org/stable/) to optimize fit parameters for the `MKSHomogenizationModel`. Lastly, the data is used to evaluate the `MKSHomogenizationModel` for effective stiffness values for a new set of microstructures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Elasticity and Effective Elastic Modulus\n",
    "\n",
    "For this example we are looking to create a homogenization linkage that predicts the effective isotropic stiffness components for two-phase microstructures. The specific stiffness component we are looking to predict in this example is $C_{xxxx}$ which is easily accessed by applying an uniaxial macroscal strain tensor (the only non-zero component is $\\varepsilon_{xx}$). \n",
    "\n",
    "$$ u(L, y) = u(0, y) + L\\bar{\\varepsilon}_{xx}$$\n",
    "\n",
    "$$ u(0, L) = u(0, 0) = 0  $$\n",
    "\n",
    "$$ u(x, 0) = u(x, L) $$\n",
    "\n",
    "More details about these boundary conditions can be found in [1]. Using these boundary conditions, $C_{xxxx}$ can be estimated calculating the ratio of the averaged stress over the applied averaged strain.\n",
    "\n",
    "$$ C_{xxxx}^* \\cong  \\bar{\\sigma}_{xx} / \\bar{\\varepsilon}_{xx}$$ \n",
    "\n",
    "In this example, $C_{xxxx}$ for 6 different types of microstructures will be estimated, using the `MKSHomogenizationModel` from `pymks`, and provides a method to compute $\\bar{\\sigma}_{xx}$ for a new microstructure with an applied strain of $\\bar{\\varepsilon}_{xx}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PYTEST_VALIDATE_IGNORE_OUTPUT\n",
    "\n",
    "from __future__ import print_function\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "A set of periodic microstructures and their volume averaged elastic stress values $\\bar{\\sigma}_{xx}$ can be generated by importing the `make_elastic_stress_random` function from `pymks.datasets`. This function has several arguments. `n_samples` is the number of samples that will be generated, `size` specifies the dimensions of the microstructures, `grain_size` controls the effective microstructure feature size, `elastic_modulus` and `poissons_ratio` are used to indicate the material property for each of the\n",
    "phases, `macro_strain` is the value of the applied uniaxial strain, and the `seed` can be used to change the the random number generator seed.\n",
    "\n",
    "Let's go ahead and create 6 different types of microstructures each with 200 samples with dimensions 21 x 21. Each of the 6 samples will have a different microstructure feature size. The function will return and the microstructures and their associated volume averaged stress values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymks.datasets import make_elastic_stress_random\n",
    "\n",
    "\n",
    "sample_size = 200\n",
    "grain_size = [(15, 2), (2, 15), (7, 7), (8, 3), (3, 9), (2, 2)]\n",
    "n_samples = [sample_size] * 6\n",
    "elastic_modulus = (310, 200)\n",
    "poissons_ratio = (0.28, 0.3)\n",
    "macro_strain = 0.001\n",
    "size = (21, 21)\n",
    "\n",
    "X, y = make_elastic_stress_random(n_samples=n_samples, size=size, grain_size=grain_size, \n",
    "                                      elastic_modulus=elastic_modulus, poissons_ratio=poissons_ratio, \n",
    "                                      macro_strain=macro_strain, seed=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array `X` contains the microstructure information and has the dimensions \n",
    "of `(n_samples, Nx, Ny)`. The array `y` contains the average stress value for \n",
    "each of the microstructures and has dimensions of `(n_samples,)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 21, 21)\n",
      "(1200,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the 6 types the microstructures to get an idea of what they \n",
    "look like. We can do this by importing `draw_microstructures`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe87c09e0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PYTEST_VALIDATE_IGNORE_OUTPUT\n",
    "X_examples = X[::sample_size]\n",
    "\n",
    "fig, axs = plt.subplots(1, X_examples.shape[0], figsize=(4 * X_examples.shape[0], 4))\n",
    "fig.subplots_adjust(right=1)\n",
    "def plot_binary(arrs,ax):\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())   \n",
    "    return ax.imshow(arrs,cmap='gray')\n",
    "\n",
    "for x in range(X_examples.shape[0]):\n",
    "    plot_binary(X_examples[x],axs[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset 4 of the 6  microstructure types have grains that are elongated in either\n",
    "the x or y directions. The remaining 2 types of samples have equiaxed grains with\n",
    "different average sizes.\n",
    "\n",
    "Let's look at the stress values for each of the microstructures shown above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stress Values [ 0.23667242  0.23284418  0.26548572  0.25849173  0.25648683  0.24744216]\n"
     ]
    }
   ],
   "source": [
    "#PYTEST_VALIDATE_IGNORE_OUTPUT\n",
    "\n",
    "print('Stress Values', y[::200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataset to work with, we can look at how to use the `MKSHomogenizationModel`to predict stress values for new microstructures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MKSHomogenizationModel Work Flow\n",
    "\n",
    "The default instance of the `MKSHomogenizationModel` takes in a dataset and \n",
    " - calculates the 2-point statistics  \n",
    " - performs [dimensionality reduction](http://en.wikipedia.org/wiki/Dimensionality_reduction) using [Prinicple Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA)  \n",
    " - and fits a [polynomial regression model](http://en.wikipedia.org/wiki/Polynomial_regression) model to the low-dimensional representation.  \n",
    "\n",
    "This work flow has been shown to accurately predict effective properties in several examples [2][3], and requires that we specify the number of components used in dimensionality reduction and the order of the polynomial we will be using for the polynomial regression. In this example we will show how we can use tools from [sklearn](http://scikit-learn.org/stable/) to try and optimize our selection for these two parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Modeling with MKSHomogenizationModel\n",
    "\n",
    "In order to make an instance of the `MKSHomogenizationModel`, we need to pass an instance of a basis (used to compute the 2-point statistics). For this particular example, there are only 2 discrete phases, so we will use the `PrimitiveBasis` from `pymks`. We only have two phases denoted by 0 and 1, therefore we have two local states and our domain is 0 to 1.\n",
    "\n",
    "Let's make an instance of the `MKSHomgenizationModel`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TwoPointcorrelation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b0409e095af6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpymks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimitive\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrimitiveTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpymks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrelations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTwoPointcorrelation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m bl=Pipeline(steps=[\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"discritize\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPrimitiveTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TwoPointcorrelation'"
     ]
    }
   ],
   "source": [
    "from pymks.fmks.bases.primitive import PrimitiveTransformer\n",
    "from pymks.fmks.correlations import TwoPointcorrelation\n",
    "\n",
    "bl=Pipeline(steps=[\n",
    "    (\"discritize\",PrimitiveTransformer(n_state=2, min_=0.0, max_=1.0)),\n",
    "    (\"Correlations\",TwoPointcorrelation(periodic_boundary=True, cutoff=21,correlations=[(0,0),(1,1)])),\n",
    "    ('flatten', FlattenTransformer()),  \n",
    "    ('reducer',PCA()),\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('connector', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the default values for the number of components and the order of the polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Default Number of Components', model.n_components)\n",
    "# print('Default Polynomail Order', model.degree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymks.fmks.correlations import TwoPointcorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These default parameters may not be the best model for a given problem; we will now show one method that can be used to optimize them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Number of Components and Polynomial Order\n",
    "\n",
    "To start with, we can look at how the variance changes as a function of the number of components.\n",
    "In general for SVD as well as PCA, the amount of variance captured in each component decreases\n",
    "as the component number increases.\n",
    "This means that as the number of components used in the dimensionality reduction increases, the percentage of the variance will asymptotically approach 100%. Let's see if this is true for our dataset.\n",
    "\n",
    "In order to do this we will change the number of components to 40 and then\n",
    "fit the data we have using the `fit` function. This function performs the dimensionality reduction and \n",
    "also fits the regression model. Because our microstructures are periodic, we need to \n",
    "use the `periodic_axes` argument when we `fit` the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_pipeline=Pipeline(steps=[\n",
    "    (\"discritize\",PrimitiveTransformer(n_state=2, min_=0.0, max_=1.0)),\n",
    "    (\"Correlations\",TwoPointcorrelation(periodic_boundary=True, cutoff=21,correlations=[(1,1),(0,1)])),\n",
    "    ('flatten', FlattenTransformer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=PCA_pipeline.fit(X).transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=40,random_state=3)\n",
    "pca.fit(A)\n",
    "variance=pca.explained_variance_\n",
    "print(variance)\n",
    "print(A[1:7])\n",
    "n_components = len(variance)\n",
    "x = np.arange(1, n_components + 1)\n",
    "plt.plot(x, np.cumsum(variance * 100), 'o-', color='#1a9641', linewidth=2)\n",
    "plt.xlabel('Number of Components', fontsize=15)\n",
    "plt.xlim(0, n_components + 1)\n",
    "plt.ylabel('Percent Variance', fontsize=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at how the cumlative variance changes as a function of the number of components using `draw_component_variance` \n",
    "from `pymks.tools`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pymks.tools import draw_component_variance\n",
    "\n",
    "\n",
    "# draw_component_variance(model.dimension_reducer.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly 93 percent of the variance is captured with the first 5 components. This means our model may only need a few components to predict the average stress.\n",
    "\n",
    "Next we need to optimize the number of components and the polynomial order. To do this we are going to split the data into test and training sets. This can be done using the [train_test_spilt](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) function from `sklearn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "flat_shape = (X.shape[0],) + (X[0].size,)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.reshape(flat_shape), y,\n",
    "                                                    test_size=0.2, random_state=3)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cross validation with the testing data to fit a number \n",
    "of models, each with a different number \n",
    "of components and a different polynomial order.\n",
    "Then we will use the testing data to verify the best model. \n",
    "This can be done using [GridSeachCV](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) \n",
    "from sklearn.\n",
    "\n",
    "We will pass a dictionary `params_to_tune` with the range of\n",
    "polynomial order `degree` and components `n_components` we want to try.\n",
    "A dictionary `fit_params` can be used to pass the `periodic_axes` variable to \n",
    "calculate periodic 2-point statistics. The argument `cv` can be used to specify \n",
    "the number of folds used in cross validation and `n_jobs` can be used to specify \n",
    "the number of jobs that are ran in parallel.\n",
    "\n",
    "Let's vary `n_components` from 1 to 11 and `degree` from 1 to 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "params_to_tune = {'reducer__n_components': np.arange(2, 12),'poly__degree': np.arange(1, 3)}\n",
    "#fit_params = {'size': X[0].shape}\n",
    "gs = GridSearchCV(bl, params_to_tune).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default `score` method for the `MKSHomogenizationModel` is the [R-squared](http://en.wikipedia.org/wiki/Coefficient_of_determination) value. Let's look at the how the mean R-squared values and their \n",
    "standard deviations change, as we varied the number of `n_components` and `degree`, using\n",
    "`draw_gridscores_matrix` from `pymks.tools`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymks.tools import draw_gridscores_matrix\n",
    "\n",
    "\n",
    "draw_gridscores_matrix(gs, ['reducer__n_components', 'poly__degree'], score_label='R-Squared',\n",
    "                       param_labels=['Number of Components', 'Order of Polynomial'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we get a poor fit, when only the first and second component are used, and when we increase\n",
    "the polynomial order and the components together. The models have a high standard deviation and \n",
    "poor R-squared values for both of these cases.\n",
    "\n",
    "There seems to be several potential models that use 4 to 11 components, but it's difficult to see which model \n",
    "is the best. Let's use our test data `X_test` to see which model performs the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Order of Polynomial',gs.best_params_.get('poly__degree'))\n",
    "print('Number of Components', gs.best_params_.get('reducer__n_components'))\n",
    "#print('Number of Components', gs.best_estimator_.n_components)\n",
    "#print('R-squared Value', np.allclose(gs.score(X_test, y_test), 1, rtol=1e-2))\n",
    "## Issue Here\n",
    "#assert gs.best_estimator_.n_components in (10, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the parameter range that we searched, we have found that a model with 2nd order polynomial \n",
    "and 11 components had the best R-squared value. Let's look at the same values, using `draw_grid_scores`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymks.tools import draw_gridscores\n",
    "\n",
    "\n",
    "gs_deg_1 = [x for x in gs.grid_scores_ \\\n",
    "            if x.parameters['poly__degree'] == 1][1:]\n",
    "gs_deg_2 = [x for x in gs.grid_scores_ \\\n",
    "            if x.parameters['poly__degree'] == 2][1:]\n",
    "# gs_deg_3 = [x for x in gs.grid_scores_ \\\n",
    "#             if x.parameters['poly__degree'] == 3][1:]\n",
    "\n",
    "draw_gridscores([gs_deg_1,  gs_deg_2,], 'reducer__n_components', \n",
    "                data_labels=['1st Order', '2nd Order'],\n",
    "                param_label='Number of Components', score_label='R-Squared')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said, a model with a 2rd order polynomial and 11 components will give us the best result. Let's use the\n",
    "best model from our grid scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gs.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using MKSHomogenizationModel\n",
    "\n",
    "Now that we have selected values for `n_components` and `degree`, lets fit the model with the data. Again, because\n",
    "our microstructures are periodic, we need to use the `periodic_axes` argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some more data that can be used to try and validate our model's prediction accuracy. We are going to\n",
    "generate 20 samples of all six different types of microstructures using the same \n",
    "`make_elastic_stress_random` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_size = 20\n",
    "n_samples = [test_sample_size] * 6\n",
    "X_new, y_new = make_elastic_stress_random(n_samples=n_samples, size=size, grain_size=grain_size, \n",
    "                                          elastic_modulus=elastic_modulus, poissons_ratio=poissons_ratio, \n",
    "                                          macro_strain=macro_strain, seed=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's predict the stress values for the new microstructures. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look to see, if the low-dimensional representation of the \n",
    "new data is similar to the low-dimensional representation of the data \n",
    "we used to fit the model using `draw_components_scatter` from `pymks.tools`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_pipeline=Pipeline(steps=[\n",
    "    (\"discritize\",PrimitiveTransformer(n_state=2, min_=0.0, max_=1.0)),\n",
    "    (\"Correlations\",TwoPointcorrelation(periodic_boundary=True, cutoff=25,correlations=[(1,1),(0,1)])),\n",
    "    ('flatten', FlattenTransformer()),  \n",
    "    ('reducer',PCA(n_components=3))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=PCA_pipeline.fit(X_train).transform(X_train)\n",
    "B=PCA_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pymks.tools import draw_components_scatter\n",
    "\n",
    "\n",
    "# draw_components_scatter([model.reduced_fit_data[:, :2], \n",
    "#                          model.reduced_predict_data[:, :2]],\n",
    "#                         ['Training Data', 'Test Data'],\n",
    "#                        legend_outside=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "title = 'Low Dimensional Representation'\n",
    "data=[A[:, :3],B[:, :3]]\n",
    "X_array = np.concatenate(data)\n",
    "component_labels = range(1,4)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('Component ' + str(component_labels[0]), fontsize=12)\n",
    "ax.set_ylabel('Component ' + str(component_labels[1]), fontsize=12)\n",
    "ax.set_zlabel('Component ' + str(component_labels[2]), fontsize=12)\n",
    "x_min, x_max = [np.min(X_array[:, 0]), np.max(X_array[:, 0])]\n",
    "y_min, y_max = [np.min(X_array[:, 1]), np.max(X_array[:, 1])]\n",
    "z_min, z_max = [np.min(X_array[:, 2]), np.max(X_array[:, 2])]\n",
    "x_epsilon = (x_max - x_min) * 0.05\n",
    "y_epsilon = (y_max - y_min) * 0.05\n",
    "z_epsilon = (z_max - z_min) * 0.05\n",
    "ax.set_xlim([x_min - x_epsilon, x_max + x_epsilon])\n",
    "ax.set_ylim([y_min - y_epsilon, y_max + y_epsilon])\n",
    "ax.set_zlim([z_min - z_epsilon, z_max + z_epsilon])\n",
    "labels=['Training Data', 'Testing Data']\n",
    "colors=['#1a9850','#f46d43']\n",
    "for label, pts,colors in zip(labels, data,colors):\n",
    "    ax.plot(pts[:, 0], pts[:, 1], pts[:, 2], 'o', label=label,color=colors)\n",
    "plt.title(title, fontsize=15)\n",
    "# ax.view_init(132, 0)\n",
    "lg = plt.legend(loc=1, borderaxespad=0., fontsize=15)\n",
    "lg = plt.legend(bbox_to_anchor=(1.05, 1.0), loc=2,borderaxespad=0., fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted data seems to be reasonably similar to the data we used to fit the model\n",
    "with. Now let's look at the score value for the predicted data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print('R-squared', np.allclose(model.score(X_new, y_new), 1, rtol=1e-2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good. Let's print out one actual and predicted stress value for each of the 6 microstructure types to see how they compare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PYTEST_VALIDATE_IGNORE_OUTPUT\n",
    "\n",
    "print('Actual Stress   ', y_new[::20])\n",
    "print('Predicted Stress', y_predict[::20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can also evaluate our prediction by looking at a goodness-of-fit plot. We\n",
    "can do this by importing `draw_goodness_of_fit` from `pymks.tools`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fit_data = np.array([y, model.predict(X)])\n",
    "pred_data = np.array([y_new, y_predict])\n",
    "y_total = np.concatenate((fit_data, pred_data), axis=-1)\n",
    "y_min, y_max = np.min(y_total), np.max(y_total)\n",
    "middle = (y_max + y_min) / 2.\n",
    "data_range = y_max - y_min\n",
    "line = np.linspace(middle - data_range * 1.03 / 2,\n",
    "                   middle + data_range * 1.03 / 2, endpoint=False)\n",
    "plt.plot(line, line, '-', linewidth=3, color='#000000')\n",
    "plt.plot(fit_data[0], fit_data[1], 'o', color='#1a9850', label=labels[0])\n",
    "plt.plot(pred_data[0], pred_data[1], 'o',\n",
    "         color='#f46d43', label=labels[1])\n",
    "plt.title('Goodness of Fit', fontsize=20)\n",
    "plt.xlabel('Actual', fontsize=18)\n",
    "plt.ylabel('Predicted', fontsize=18)\n",
    "plt.legend(loc=2, fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `MKSHomogenizationModel` has created a homogenization linkage for the effective stiffness for the 6 different microstructures and has predicted the average stress values for our new microstructures reasonably well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Landi, G., S.R. Niezgoda, S.R. Kalidindi, Multi-scale modeling of elastic response of three-dimensional voxel-based microstructure datasets using novel DFT-based knowledge systems. Acta Materialia, 2009. 58 (7): p. 2716-2725 [doi:10.1016/j.actamat.2010.01.007](http://dx.doi.org/10.1016/j.actamat.2010.01.007).\n",
    "\n",
    "[2] Çeçen, A., et al. \"A data-driven approach to establishing microstructure–property relationships in porous transport layers of polymer electrolyte fuel cells.\" Journal of Power Sources 245 (2014): 144-153. [doi:10.1016/j.jpowsour.2013.06.100](http://dx.doi.org/10.1016/j.jpowsour.2013.06.100)\n",
    "\n",
    "[3] Deshpande, P. D., et al. \"Application of Statistical and Machine Learning Techniques for Correlating Properties to Composition and Manufacturing Processes of Steels.\" 2 World Congress on Integrated Computational Materials Engineering. John Wiley & Sons, Inc.  [doi:10.1002/9781118767061.ch25](http://dx.doi.org/10.1002/9781118767061.ch25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

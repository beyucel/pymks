{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effective Stiffness of Fiber Composite\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This example demonstrates the use of the homogenization model from PyMKS on a set of fiber-like structures.  These structures are simulated to emulate fiber-reinforced polymer samples.  For a summary of homogenization theory and its use with effective stiffness properties please see the [Effective Siffness example](http://materialsinnovation.github.io/pymks/rst/stress_homogenization_2D.html). This example will first generate a series of random microstructures with various fiber lengths and volume fraction.  The ability to vary the volume fraction is a new functionality of this example.  Then the generated stuctures will be used to calibrate and test the model based on simulated effective stress values.  Finally we will show that the simulated response compare favorably with those generated by the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (correlations.py, line 173)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/nix/store/l82bgwgzhyc6sls20g9kf0r6as3i6zzs-python3.7-ipython-7.10.2/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3319\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-188f17d0c35b>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from pymks.fmks.correlations import auto_correlation\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/berkay/Projects/pymks/pymks/fmks/correlations.py\"\u001b[0;36m, line \u001b[0;32m173\u001b[0m\n\u001b[0;31m    nonperiodic_padder =\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# NBVAL_IGNORE_OUTPUT\n",
    "\n",
    "from __future__ import print_function\n",
    "import pymks\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pymks.fmks.correlations import auto_correlation\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are definitions for parameters used to create the microstructures.  `n_samples` determines how many microstructures of a particular volume fraction to create.  `size` determines the number of pixels to include in the microstructure.  We define the material properties used in the finite element simulation with `elastic_modulus`, `poissons_ratio` and `macro_strain`.  `n_phases` and `grain_size` determine the physical characteristics of the microstructure.  We use a high aspect ratio of grain shape to create our microstructures to simulate fiber-like structures.  The `v_frac` variable determines the fraction of each phase.  The sum of the volume fractions must be equal to 1.  The `percent_variance` variable introduces variation into the volume fraction up to the specified percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100\n",
    "n_samples = 4 * [sample_size]\n",
    "size = (101, 101)\n",
    "elastic_modulus = (1.3, 75)\n",
    "poissons_ratio = (0.42, .22)\n",
    "macro_strain = 0.001\n",
    "n_phases = 2\n",
    "grain_size = [(40, 2), (10, 2), (2, 40), (2, 10)]\n",
    "v_frac = [(0.7, 0.3), (0.6, 0.4), (0.3, 0.7), (0.4, 0.6)]\n",
    "percent_variance = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create the microstructures and generate their responses using the `make_elastic_stress_random` function from PyMKS.  Four datasets are used to generate the four different volume fractions simulated.  Then the datasets are combined into one variable. The volume fractions are listed in the variable `v_frac`.  Variation around the specified volume fraction is obtained by varying `percent_variance`.  The variation is randomly generated according to a uniform distribution around the specified volume fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_IGNORE_OUTPUT\n",
    "\n",
    "from pymks.datasets import make_elastic_stress_random\n",
    "\n",
    "\n",
    "dataset, stresses = make_elastic_stress_random(n_samples=n_samples, size=size, grain_size=grain_size,\n",
    "                                                 elastic_modulus=elastic_modulus, poissons_ratio=poissons_ratio,\n",
    "                                                 macro_strain=macro_strain, volume_fraction=v_frac,\n",
    "                                                 percent_variance=percent_variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are a few microstructres to look at how the fiber length, orientation and volume fraction are varied.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = dataset[::sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, examples.shape[0], figsize=(4 * examples.shape[0], 4))\n",
    "fig.subplots_adjust(right=1)\n",
    "\n",
    "def plot_binary(arrs,ax):\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())   \n",
    "    return ax.imshow(arrs,cmap='gray')\n",
    "\n",
    "for x in range(examples.shape[0]):\n",
    "    plot_binary(examples[x],axs[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "\n",
    "Next we create the the homogenization pipeline. The sklearn pipelines assemble multiple steps together that can be cross-validated together (with different parameters). The Pipeline below (`bl`)  takes in microstructures, discritizes the microstructures and runs two-point statistics to get a statistical representation of the microstructures.  An explanation of the use of two-point statistics can be found in the [Checkerboard Microstructure Example](http://materialsinnovation.github.io/pymks/rst/checker_board.html).  Then the model uses PCA and regression models to create a linkage between the calcualted properties and structures.  \n",
    "Here we simply initiate the homogenization pipeline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymks.fmks.bases.primitive import PrimitiveTransformer\n",
    "from pymks.fmks.correlations import TwoPointcorrelation,FlattenTransformer\n",
    "\n",
    "bl = Pipeline(steps=[\n",
    "    (\"discritize\",PrimitiveTransformer(n_state=2, min_=0.0, max_=1.0)),\n",
    "    (\"Correlations\",TwoPointcorrelation(periodic_boundary=True, cutoff=25,correlations=[(1,1),(0,1)])),\n",
    "    ('flatten', FlattenTransformer()),  \n",
    "    ('reducer',PCA()),\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('connector', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into testing and training segments to see if the model accurately predicts the effective stress. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, stress_train, stress_test = train_test_split(\n",
    "    dataset, stresses, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sklearn's GridSearchCV to optimize the `n_components` and `degree` hyper-parameters for the model. Let's search over the range of 1st order to 3rd order degree polynomials and 2 to 7 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params_to_tune = {'reducer__n_components': np.arange(2, 7),'poly__degree': np.arange(1, 4)}\n",
    "\n",
    "gs = GridSearchCV(bl, cv=3, param_grid=params_to_tune, iid=True, return_train_score=True).fit(data_train, stress_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Order of Polynomial',gs.best_params_.get('poly__degree'))\n",
    "print('Number of Components', gs.best_params_.get('reducer__n_components'))\n",
    "print('R-squared Value', np.allclose(gs.score(data_test, stress_test), 0.9985, rtol=1e-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz.curried import groupby, get, pipe, valmap, merge_with, merge, valmap, pluck\n",
    "from toolz.curried import map as map_\n",
    "from pymks.fmks.func import sequence\n",
    "\n",
    "\n",
    "def compute(data):\n",
    "    \"\"\"Compute the component, error and standard deviation arrays from a grouped\n",
    "    grid score.\n",
    "    \"\"\"\n",
    "    pluck_ = lambda x: list(pluck(x, data))\n",
    "    return dict(\n",
    "        std_dev=pluck_('std_test_score'),\n",
    "        mean=np.array(pluck_('mean_test_score')),\n",
    "        x=pluck_('param_reducer__n_components')\n",
    "    )\n",
    "\n",
    "def munge(data, plot_data):\n",
    "    \"\"\"Compute the data for plotting from the grid scores\n",
    "    \"\"\"\n",
    "    return pipe(\n",
    "        data,\n",
    "        groupby(lambda x: x['params']['poly__degree']),\n",
    "        valmap(compute),\n",
    "        lambda x: merge_with(merge, plot_data, x)\n",
    "    )\n",
    "\n",
    "def plot_line(x, mean, std_dev, color, label):\n",
    "    plt.fill_between(x, mean - std_dev, mean + std_dev, alpha=0.1, color=color)\n",
    "    plt.plot(x, mean, 'o-', color=color, label=label, linewidth=2)\n",
    "    \n",
    "\n",
    "def plot(grid_scores, plot_data):\n",
    "    valmap(lambda x: plot_line(**x), munge(grid_scores, plot_data))\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,fontsize=15)\n",
    "    plt.ticklabel_format(style='sci', axis='y')\n",
    "    plt.ylabel('R-Squared', fontsize=20)\n",
    "    plt.xlabel('Number of Components', fontsize=20)\n",
    "    plt.show()\n",
    "    \n",
    "plot(\n",
    "    pandas.DataFrame(gs.cv_results_).to_dict('records'),\n",
    "    {\n",
    "        1: dict(label='1st Order', color='#1a9850'),\n",
    "        2: dict(label='2nd Order', color='#f46d43'),\n",
    "        3: dict(label='3rd Order', color='#1f78b4')\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best model was found to have `degree` equal to 2 and `n_components` equal to 5. Let's go ahead and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structures in PCA space\n",
    "\n",
    "Now we want to draw how the samples are spread out in PCA space and look at how the testing and training data line up. To do that we initiate a new pipeline (`PCA_pipeline`) that will perform discretization, 2-point correlation representation, and dimensionality reduction on microstructures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_predict = model.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_pipeline = Pipeline(steps=[\n",
    "    (\"discritize\",PrimitiveTransformer(n_state=2, min_=0.0, max_=1.0)),\n",
    "    (\"Correlations\",TwoPointcorrelation(periodic_boundary=True, cutoff=25,correlations=[(1,1),(0,1)])),\n",
    "    ('flatten', FlattenTransformer()),  \n",
    "    ('reducer',PCA(n_components=3))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply `PCA_pipeline` to the training dataset and testing dataset to obtain their principle component values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the dimensionally reduced microstructure information in the 3D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(trained_pca_scores, test_pca_scores):\n",
    "    \n",
    "    # required to change Matplotlib context to get 3D working\n",
    "    # although Axes3D is never used\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.set_xlabel('Component 1', fontsize=12)\n",
    "    ax.set_ylabel('Component 2', fontsize=12)\n",
    "    ax.set_zlabel('Component 3', fontsize=12)\n",
    "    ax.plot(*trained_pca_scores.T, 'o', label='Training Data', color='#1a9850')\n",
    "    ax.plot(*test_pca_scores.T, 'o', label='Test Data', color='#f46d43')\n",
    "    plt.title('Low Dimensional Representation', fontsize=15)\n",
    "    ax.view_init(-132, 80)\n",
    "    plt.legend(loc=1, borderaxespad=0., fontsize=15)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc=2,borderaxespad=0., fontsize=15)\n",
    "    plt.show()\n",
    "    \n",
    "plot(\n",
    "    PCA_pipeline.fit(data_train).transform(data_train),\n",
    "    PCA_pipeline.transform(data_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is pretty good agreement between the testing and the training data.  We can also see that the four different fiber sizes are seperated in the PC space.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Goodness of fit\n",
    "\n",
    "Let's see how well the model predicts the structure properties. The calculated properties are plotted against the properties generated by the model. We see a linear relationship with a slope of 1.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ranges = sequence(\n",
    "    lambda x: np.concatenate(x, axis=-1),\n",
    "    lambda x: [np.min(x) * 0.9, np.max(x) * 1.03],\n",
    "    lambda x: (x, x)\n",
    ")\n",
    "\n",
    "def plot(fit_data, pred_data):\n",
    "    plt.plot(*get_ranges([fit_data, pred_data]), linewidth=3, color='#000000')\n",
    "    out2 = plt.plot(*fit_data, 'o', color='#1a9850', label='Training Data')\n",
    "    out1 = plt.plot(*pred_data, 'o', color='#f46d43', label='Test Data')\n",
    "    plt.title('Goodness of Fit', fontsize=20)\n",
    "    plt.xlabel('Actual', fontsize=18)\n",
    "    plt.ylabel('Predicted', fontsize=18)\n",
    "    plt.legend(loc=2, fontsize=15)\n",
    "    plt.show()\n",
    "    \n",
    "plot(\n",
    "    np.array([stresses, model.predict(dataset)]),\n",
    "    np.array([stress_test, stress_predict])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There is a good correlation between the FE results and those predicted by the linkage.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
